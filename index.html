
<!DOCTYPE html>
<!-- Thanks to url=https://www.aicitychallenge.org/ -->
<!-- TODO: -->
<!--  complete WORKSHOP pages -->


<html lang="en-US" class="js svg background-fixed">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CRDH – 2020 IEEE AIVR WORKSHOP</title>
  <script src="https://www.w3schools.com/lib/w3.js"></script> 

  <link rel="profile" href="http://gmpg.org/xfn/11">
  <link rel="dns-prefetch" href="https://fonts.googleapis.com/">
  <link rel="dns-prefetch" href="https://s.w.org/">
  <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
  <link rel="stylesheet" id="wp-quicklatex-format-css" href="css/quicklatex-format.css" media="all">
  <link rel="stylesheet" id="sdm-styles-css" href="css/sdm_wp_styles.css" media="all">
  <link rel="stylesheet" id="uaf_client_css-css" href="css/uaf.css" media="all">
  <link rel="stylesheet" id="twentyseventeen-fonts-css" href="css/font_latin.css" media="all">
  <link rel="stylesheet" id="twentyseventeen-style-css" href="css/style.css" media="all">
  <link rel="stylesheet" id="font-awesome-css" href="css/font-awesome.min.css" media="all">
  <link rel="stylesheet" id="elementor-animations-css" href="css/animations.min.css" media="all">
  <link rel="stylesheet" id="elementor-frontend-css" href="css/frontend.min.css" media="all">
  <link rel="stylesheet" id="elementor-global-css" href="css/global.css" media="all">
  <link rel="stylesheet" id="elementor-icons-css" href="css/elementor-icons.min.css" media="all">
  <link rel="stylesheet" id="elementor-post-11-css" href="css/post-11.css" media="all">
  <link rel="stylesheet" id="google-fonts-1-css" href="css/font_cy.css" media="all">

</head>

<body class="twentyseventeen-front-page has-header-video">

<div id="HOME" class="centerContainer">
  <!-- header -->
  <header id="header" class="site-header" role="banner">

    <div class="custom-header" style="margin-bottom: 72px;">

      <!-- Video -->
      <!-- e3r-gVIf-qc.mp4, QYDY_8lL89E.mp4 -->
      <!-- https://developers.google.com/youtube/player_parameters -->
      <div class="custom-header-media">
        <div id="myvideo">
  <img class="trailer" src="img/slideshow/realistic_1.jpg">
  <img class="trailer" src="img/slideshow/realistic_2.jpg">
  <img class="trailer" src="img/slideshow/realistic_3.jpg">
  <img class="trailer" src="img/slideshow/realistic_4.jpg">
  <img class="trailer" src="img/slideshow/interdigital.jpg">
  <img class="trailer" src="img/slideshow/interdigital-2.jpg">
  <img class="trailer" src="img/slideshow/interdigital-3.jpg">
  <script>
	w3.slideshow(".trailer", 3000);
  </script>
</div>
      </div>

      <div class="site-branding">
        <div class="wrap">
          <h1 class="site-title"><a href="#INTRODUCTION" rel="home">CRDH : From Capture to Rendering of Digital Humans for AR/VR</a></h1>
          <p class="site-description">December 16 @ 3rd IEEE International Conference on AIVR 2020 in Utrecht, The Netherlands.</p>
        </div><!-- .wrap -->
      </div><!-- .site-branding -->

    </div><!-- .custom-header -->

    <div class="navigation-top">
      <div class="wrap">
        <nav id="site-navigation" class="main-navigation">

          <!-- Floating menu -->
          <button class="menu-toggle">
            Menu
          </button>

          <ul id="top-menu" class="menu">
            <li id="homeitem" class="menu-item"><a href="#HOME">HOME</a></li>
            <li id="introductionitem" class="menu-item"><a href="#INTRODUCTION">INTRODUCTION</a></li>
            <li id="newsitem" class="menu-item"><a href="#NEWS">NEWS</a></li>
            <li id="locationsitem" class="menu-item"><a href="#LOCATION">REGISTRATION & CONNECTION</a></li>
            <!--<li id="papersitem" class="menu-item"><a href="#SUBMISSION">SUBMISSION</a></li>
            <li id="rightsitem" class="menu-item"><a href="#RIGHTS">RIGHTS</a></li> -->
            <li id="workshopitem" class="menu-item"><a href="#WORKSHOPPROGRAM">PROGRAM</a></li>
            <!--<li id="datesitem" class="menu-item"><a href="#IMPORTANTDATES">DATES</a></li>-->
            <!--
            <li id="posteritem" class="menu-item"><a href="#POSTERS">POSTERS</a></li>
            -->

            <li id="submenu" class="menu-item-has-children">
              <a>INFO</a>
              <ul class="sub-menu">
		<li id="speakersitem" class="menu-item"><a href="#KEYNOTESPEAKERS">Keynote Speakers</a></li>
                <li id="speakersitem" class="menu-item"><a href="#INVITEDSPEAKERS">Invited Speakers</a></li>
                <li id="committeeitem" class="menu-item"><a href="#ORGANIZINGCOMMITTEES">Organizers</a>
                </li>
                <li id="contactitem" class="menu-item"><a href="#CONTACT">Contact Us</a></li>
              </ul>
            </li>

          </ul>

        </nav><!-- #site-navigation -->
      </div><!-- .wrap -->
    </div><!-- .navigation-top -->

  </header><!-- #masthead -->

  <!-- content -->
  <div class="site-content-contain">
<!--    style="background-image: url('img/background.png');-->
<!--			background-attachment: scroll; background-size: 100%; background-repeat: repeat-y; background-position:-->
<!--			center;">-->
    <div class="elementor elementor-11" style="margin: 0 5% 0 5%">

      <!-- Introduction -->
      <div>
        <div>
          <h2 id="INTRODUCTION" class="elementor-heading-title">
            INTRODUCTION</h2>
        </div>
        <div class="elementor-widget-text-editor">
          <div class="elementor-text-editor">
Realistic representation of digital humans in AR/VR applications is only made possible with the capture of high-quality data and appropriate rendering techniques. While capturing of accurate and relightable data is required to produce assets for realistic avatars, we also need real-time performance capture to ensure success of applications like teleconference and teleportation. On the other hand, rendering of photo-real humans is even more important to the immersive experience in virtual scenes. This workshop provides a platform to share some of the most advanced human face/body capturing systems from pore-level high resolution capture to rapid motion capture along with the art of data processing. It will also cover novel rendering along with environment lighting estimation techniques required in AR/VR, like neural rendering.
			
            <br><br>
After a successful first edition, we are excited to present a second edition of the workshop. We expect that this event will inspire novel ideas based on current practices in the field of rendering realistic digital humans and accelerate the hardware and software development in the same field. A main objective for this workshop is to bring researchers potential collaboration opportunities. It will also provide a good introduction for researchers interested in the field.

            <br><br>
          </div>
        </div>
      </div>
      
      <!-- NEWS -->
      <div>
        <div>
          <h2 id="NEWS" class="elementor-heading-title">
            NEWS</h2>
        </div>
        <div class="elementor-widget-text-editor">
          <div class="elementor-text-editor">
            [12/09/2020] Minor change in the agenda. Unfortunately Marco Romeo will not be able to attend the workshop. Gerard Pons-Moll will be the keynote speaker of the first session.
            <br><br>
          </div>
          <div class="elementor-text-editor">
            [11/01/2020] Our two fantastic keynote speakers are Wan-Chun Alex Ma and Marco Romeo!
            <br><br>
          </div>
          <!-- <div class="elementor-text-editor">
            [07/16/2020] <a href="https://easychair.org/my/conference?conf=ieeeaivr2020">Submission website</a> is opened
            <br><br>
          </div> -->
          <div class="elementor-text-editor">
            [07/08/2020] The workshop will be a <a href="https://aivr.science.uu.nl/aivr_message.html">live event</a>
            <br><br>
          </div>
          <div class="elementor-text-editor">
            [06/01/2020] Website for the edition 2020 is online!
            <br><br>
          </div>
        </div>
      </div>
      
      <!-- Location -->
      <div>
        <div>
          <h2 id="LOCATION" class="elementor-heading-title">
            REGISTRATION & CONNECTION</h2>
        </div>
        <div class="elementor-widget-text-editor">
          <div class="elementor-text-editor">
            The workshop will take place during the IEEE 3rd International Conference on Artificial Intelligence & Virtual Reality (AIVR 2020). Due to the COVID-19 situation, the conference and the workshop will be held fully online. Information regarding the conferencing platform will be provided soon. <b><a href=https://aivr.science.uu.nl/aivr_registration.html>Registration</a></b> must be done from the main conference website.
            <br><br>
          </div>
        </div>
      </div>

      <!-- Call for Papers -->
      <!-- <div>
        <div>
          <h2 id="SUBMISSION" class="elementor-heading-title">SUBMISSION</h2>
        </div>


        <div class="elementor-widget-text-editor">
          <div>
            Authors are invited to submit a maximum <b>4 pages</b> technical workshop paper in double-column IEEE format following the official <b><a
              href="https://www.ieee.org/conferences/publishing/templates.html"
              target="_blank">IEEE Manuscript Formatting guidelines.</a></b>
			All submissions will go through a double-blind peer-review process. Authors of accepted papers are expected to attend the conference and present their paper at the workshop.
  

            <br><br>

            <b>All the papers should be submitted using <a href="https://easychair.org/my/conference?conf=ieeeaivr2020"><u>EasyChair website</u> under the track of <i>"IEEE AIVR 2020 - Workshop on From Capture to Rendering of Digital Humans for AR/VR"</a></i></b>
		  <br><br>


            The topic should includes but not limited to:

            <br><br>

            <div style="margin: 0 50px">
              <ul>
                <li>Capture systems (hardware / software)</li>
                <li>Creation of Digital Humans</li>
                <li>Rendering of Digital Humans</li>
                <li>Motion capture</li>
                <li>AR/VR experience with Digital Humans</li>
                <li>Anything related to Digital Humans</li>
              </ul>
            </div>

          </div>
        </div>
      </div>
      Accepted workshop papers and special session papers will be published in the conference proceedings by IEEE Computer Society Press and included in the IEEE Xplore Digital Library.
      <br><br>
	  -->
	  
	  <!-- Rights -->
      <!--<div>
        <div>
          <h2 id="RIGHTS="elementor-heading-title">RIGHTS</h2>
        </div>
        <div class="elementor-widget-text-editor">
          <div class="elementor-text-editor">
            The work submitted to the conference is subject to the <b><a
              href="https://www.ieee.org/publications/rights/index.html"
              target="_blank">IEEE Intellectual Property Rights</a></b> and  <b><a
              href="https://www.ieee.org/publications/rights/copyright-policy.html"
              target="_blank">Copyright policy</a></b>.
            Please read carefully the linked webpages before submitting a contribution.
            <br><br>
          </div>
        </div>
      </div>
-->
      
	  <!-- Important Dates -->
      <!--<div>
        <div>
          <h2 id="IMPORTANTDATES" class="elementor-heading-title">IMPORTANT DATES</h2>
        </div>

        <div class="elementor-widget-text-editor">
          <div style="text-align: center">
            <table class="tg">
              <tr>
                <!--<td>Paper submission deadline:</td>
                <td><b><s>September 24<sup>th</sup></s>, <span style="color:#FF0000";>October 4<sup>th</sup></span>,  2020</b>
                </td>
              </tr>
              <tr>
                <td>Notification of acceptance:</td>
                <td><b>October 16<sup>th</sup>, 2020</b></td>
              </tr>
              <tr>
                <td>Camera-ready Deadline:</td>
                <td><b>October 30<sup>th</sup>, 2020</b></td>
              </tr>-->
             <!-- <tr>
                <td>Workshop date:</td>
                <td><b>December 16<sup>th</sup>, 2020
               </b></td>
              </tr>
            </table>
          </div>
        </div>
      </div> -->

      <!-- Workshop Program -->
      <div>
        <div class="elementor-widget-container">
          <h2 id="WORKSHOPPROGRAM" class="elementor-heading-title elementor-size-default">
            WORKSHOP PROGRAM</h2>
        </div>
	<div class="elementor-widget-text-editor">
          <div class="elementor-text-editor">
            <!--<b>TBD</b> <br /> The program of the previous edition can be found on the <a href="https://aivr2019.github.io/CRDH-workshop/">CRDH 2019 website</a>
            <br><br>
          </div>-->

          <b>Workshop Date: December 16, 2020</b><br/>
							  
	 <b>1:30pm - 3:30pm CET Session 1: Workshop CRHD (part 1)</b>
             <div style="margin: 0 50px">
              <ul>
              
                <li>1:30 - 1:40 Introduction</li>
                <li>1:40 - 2:30 Keynote Speaker: Gerard Pons-Moll</li>
                <li>2:30 - 3:00 Invited Talk: Adnane Boukhayma</li>
                <li>3:00 - 3:30 Invited Talk: Yajie Zhao</li> 
	      </ul>
            </div>
          <b>5:30 - 7:30 CET Session 3: Workshop CRHD (part 2)</b>       
	<div style="margin: 0 50px">
              <ul>
		<li>5:30 - 5:40 Introduction</li>
                <li>5:40 - 6:30 Keynote Speaker: Wan-Chun Alex Ma</li>
                <li>6:30 - 7:00 Invited Talk: Koki Nagano</li>
                <li>7:00 - 7:30 Invited Talk: Mario Botsch</li>
               </ul>
            </div>						  
       </div>
       
      <!-- Keynote Speakers -->
      <div>
        <div>
          <h2 id="KEYNOTESPEAKER" class="elementor-heading-title">KEYNOTE SPEAKERS</h2>
        </div>

        <div class="elementor-widget-text-editor">
          <div class="elementor-text-editor">

          <div style="text-align: left">
            <table class="tg">
              <tr>
               <td class="elementor-widget-image-box">
                <div class="elementor-image-box-wrapper">

                  <a href="https://www.linkedin.com/in/wmafx/" target="_blank">
                   <div class="elementor-image-box-img">
                      <img style="margin: auto; height: 150px; max-width: 150px;border-radius: 50%;"
                           src="img/alex.jpg" alt=""> 
					
                 </div></td>
                <td><b>Wan-Chun Alex Ma</b><br/>
							  <b>Title: </b>Realism for Digital Faces: Modeling Appearance and Motion<br/>
		<b>Bio: </b>Wan-Chun Alex Ma is a senior research scientist at ByteDance. His research interests include: digital human, facial animation, performance capture, photogrammetry, image-based modeling and rendering, geometry processing, and machine learning. He has been affiliated with Google, Activision Blizzard, ETH Zurich, Weta Digital, and USC Institute for Creative Technologies. His career has spanned more than a decade across many fields including visual effects, video game development, and augmented reality. As a Senior Software Engineer at Google, he led the research and development of a new lighting estimation technique that promotes realism in augmented reality. He also held research scientist positions at both Weta Digital and Activision Blizzard, where his work on facial performance capture contributed to motion pictures and video games, including the Hobbit Trilogy, Iron Man 3, the recent Planet of the Apes series, Furious 7, The BFG, and the Call of Duty series. His works have been published at conferences such as SIGGRAPH, SIGGRAPH Asia, Eurographics, EGSR, SGP, and I3D. Ma received his Ph.D. degree from National Taiwan University in 2008. His dissertation on the Polarized Spherical Gradient Illumination facial appearance capture system has been used extensively for creating photoreal digital actors by the visual effects industry. This work was later recognized by the AMPAS with the Academy Award for Technical Achievement in 2019.</br>

							  <b>Abstract: </b>Nowadays digital humans are widely adopted in emerging areas such as extended reality, virtual production and gaming. We have
witnessed in various applications that creating a completely realistic digital human face is not an impossible task anymore. In this talk we will examine technical key issues and solutions on modeling the appearance and motion of digital faces in the history of digital face creation, along with examples from prior feature film productions or academia works. We will also look at what are the next areas we should address more for increasing the realism for digital faces in the near
future.<br/>
                </td>
              </tr>
                  <tr>
               <td class="elementor-widget-image-box">
                <div class="elementor-image-box-wrapper">

                  <a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank">
                   <div class="elementor-image-box-img">
                      <img style="margin: auto; height: 150px; max-width: 150px;border-radius: 50%;"
                           src="img/gerard.png" alt=""> 
					
                 </div></td>
                <td><b>Gerard Pons-Moll</b><br/>
							  <b>Title: </b>Democratizing
 Digital Humans<br/>
			<b>Bio: </b>Pons-Moll is the head of the Emmy Noether independent research group "Real Virtual Humans", senior researcher at the Max Planck for Informatics (MPII) in Saarbrücken, Germany, and will join the University of Tübingen as full professor in March of 2021.
His research lies at the intersection of computer vision, computer graphics and machine
 learning -- with special focus on analyzing people in videos, and creating virtual human models by "looking" at real ones. His research has produced some of the most advanced statistical human body models of pose, shape, soft-tissue and clothing (which are
 currently used for a number of applications in industry and research), as well as algorithms to track and reconstruct 3D people models from images, video, depth, and IMUs.
His work has received several awards including the prestigious Emmy Noether
 Grant (2018), a Google Faculty Research Award (2019), a Facebook Reality Labs Faculty Award (2018), and recently the German Pattern Recognition Award (2019), which is given annually by the German Pattern Recognition Society to one outstanding researcher in
 the fields of Computer Vision and Machine Learning. His work got Best Papers Awards BMVC’13, Eurographics’17, 3DV'18 and CVPR'20 and has been published at the top venues and journals including CVPR, ICCV, Siggraph, Eurographics, 3DV, IJCV and PAMI. He served
 as Area Chair for the major conferences in vision such as ECCV, CVPR, 3DV, etc<br />
							  <b>Abstract: </b>Digitizing
 human beings would redefine the way we think and communicate (with other humans and with machines), and it is necessary for many applications -- for example, to transport people into virtual and augmented reality, for entertainment and special effects in movies,
 and for medicine and psychology. 

Digital humans obtained with state of the art VFX are of astonishing realism
 and quality. However, generating them is a time consuming and expensive process which requires specialised equipment, expert knowledge and manual labor, limiting their use to specialists.
In this talk, I will describe the work we are conducting in my lab to capture and
 learn digital humans from consumer grade sensors like RGB cameras, which are usable and accessible to everyone.
<br/>
                </td>
              </tr>
                   
            </table>
          </div>
        </div>
      </div>
<!-- Invited Speakers -->
      <div>
        <div>
          <h2 id="INVITEDSPEAKER" class="elementor-heading-title">INVITED SPEAKERS</h2>
        </div>

        <div class="elementor-widget-text-editor">
          <div class="elementor-text-editor">

          <div style="text-align: left">
            <table class="tg">
                   <tr>
               <td class="elementor-widget-image-box">
                <div class="elementor-image-box-wrapper">

                  <a href="https://boukhayma.github.io/" target="_blank">
                   <div class="elementor-image-box-img">
                      <img style="margin: auto; height: 150px; max-width: 150px;border-radius: 50%;"
                           src="img/adnane.png" alt=""> 
					
                 </div></td>
                <td><b>Adnane Boukhayma</b><br/>
							  <b>Title: </b>Learning to reconstruct human parts from a single image with limited paired training data<br/>
				<b>Bio: </b>Adnane Boukhayma is a tenured research scientist (chargé de recherche) at Inria Rennes specializing in 3D computer vision and deep learning, with a focus on human capture and synthesis. Previously, he was a postdoctoral researcher at the University of Oxford in the Torr Vision Group lead by Prof. Philip Torr. He obtained his PhD at Inria Grenooble under the supervision of Dr. Edmond Boyer. <br/>
							  <b>Abstract: </b>3D reconstruction of humans and their various body parts is key in enabling numerous applications including i.e. human digitization, XR and human machine interaction. In this respect, learning based approaches have recently allowed to lower the acquisition constraints, and we are able to build systems that can obtain reliable reconstructions from as much as a commodity monocular color input, especially for class specific tasks. These are mostly deep learning networks that rely on learning strong statistical priors given enough training data. For 3D prediction tasks, obtaining substantial amounts of training images in the wild paired with 3D ground-truth can be challenging. We show here how to deal with this issue by exploiting other forms of data such as pre-built parametric models or multi-modal unpaired data within a deep learning framework. <br/>
                </td>
              </tr>
                   <tr>
               <td class="elementor-widget-image-box">
                <div class="elementor-image-box-wrapper">

                  <a href="https://graphics.uni-bielefeld.de/people/botsch_mario/botsch_mario.html" target="_blank">
                   <div class="elementor-image-box-img">
                      <img style="margin: auto; height: 150px; max-width: 150px;border-radius: 50%;"
                           src="img/mario.png" alt=""> 
					
                 </div></td>
                <td><b>Mario Botsch</b><br/>
							  <b>Title: </b>Reconstructing Realistic Avatars with(out) a Complex Photogrammetry Scanner<br/>
				<b>Bio: </b>Mario Botsch is professor for Computer Science at TU Dortmund University, where he is heading the Chair of Computer Graphics. The focus of his research is the efficient acquisition, optimisation, animation, and visualisation of 3D geometric objects. He is currently investigating 3D-scanning and motion capturing of humans, modelling and animation of virtual characters, and real-time visualisation in interactive virtual reality scenarios. <br/>
							  <b>Abstract: </b>Generating realistic virtual avatars used to require (1) a complex photogrammetry scanner consisting of many DSLR cameras as well as (2) a time-consuming and labour-intensive reconstruction and rigging process. In this talk I will present our recent efforts for reducing both requirements. First, I will show how to reconstruct realistic avatars in less than 10 minutes with only a small amount of manual work, however, still using a complex photogrammetry setup. In the second part, we reduce hardware costs by two orders of magnitude by replacing the multi-camera photogrammetry rig by simple monocular RGB videos taken with a standard smartphone. From those videos we reconstruct realistic avatars in a fully automatic manner, in just about 20 minutes, and at a visual quality close to the complex scanner setup.<br/>
                </td>
              </tr>
                   <tr>
               <td class="elementor-widget-image-box">
                <div class="elementor-image-box-wrapper">

                  <a href="https://luminohope.org/" target="_blank">
                   <div class="elementor-image-box-img">
                      <img style="margin: auto; height: 150px; max-width: 150px;border-radius: 50%;"
                           src="img/koki.jpg" alt=""> 
					
                 </div></td>
                <td><b>Koki Nagano</b><br/>
							  <b>Title: </b>AI-Driven Photorealistic Human Digitization<br/>
				<b>Bio: </b>Koki Nagano is a Senior Research Scientist at NVIDIA Research. He works at the intersection of Graphics and AI with focus on achieving realistic digital humans. He has worked on a 3D display that allows an interactive conversation with a holographic projection of Holocaust survivors to preserve visual archives of the testimonies for future classrooms. His work on skin microgeometry synthesis has helped create digital characters in blockbuster movies such as “Ready Player One” and “Blade Runner 2049” as well as the open source ones such as “Digital Mike” and “Digital Emily 2.0”. His work on photorealistic human digitization has been shown in places including World Economic Forum, EmTech, TEDxCharlottesville, and SIGGRAPH Real-time Live!. His work has also led to the development of the state of the art Deepfake detection technology in collaboration with top media forensics experts. He was named a Google PhD Fellow 2016 and his research has won the DC Expo 2015 Special Prize. He previously worked for Pinscreen as Principal Scientist. He obtained his PhD from the University of Southern California advised by Dr. Paul Debevec at USC ICT and his BE from the Tokyo Institute of Technology.<br/>
							  <b>Abstract: </b>It is unquestionable that photorealistic digital humans will become ubiquitous in society, whether in the form of AI assistants or as fictional characters on viral media or as our own virtual self for social interactions. While currently creating a convincing digital human involves an expensive and lengthy procedure from a team of VFX experts, in the near future, anyone will be able to create photorealistic human content at their fingertips. In this talk, I present techniques to create photorealistic digital humans using 3D computer graphics and deep learning. Using the Light Stage high-fidelity capture systems, I describe how we can achieve realistic rendering of an animated face in real-time that is accurate to the level of microns. By combining cutting edge 3D graphics and deep generative models, I present methods to model, animate, and render photorealistic 3D humans from minimal inputs to bring avatar digitization to everyone. In particular I will showcase a deep generative model that can synthesize and animate in real-time a photorealistic 3D face of anyone from a single selfie. While these are key  technologies for the creation of consumer accessible virtual beings, they can also be misused for malicious purposes such as the spread of disinformation. To that end, I will discuss a method to detect advanced media forgeries such as deepfakes and our efforts to fight against them. <br/>
                </td>
              </tr>
                   <tr>
               <td class="elementor-widget-image-box">
                <div class="elementor-image-box-wrapper">

                  <a href="https://www.yajie-zhao.com/" target="_blank">
                   <div class="elementor-image-box-img">
                      <img style="margin: auto; height: 150px; max-width: 150px;border-radius: 50%;"
                           src="img/yajie.jpg" alt=""> 
					
                 </div></td>
                <td><b>Yajie Zhao</b><br/>
							  <b>Title: </b>Next Generation lifelike Avatar Creation<br/>
				<b>Bio: </b>Yajie Zhao is a computer scientist at USC Institute for Creative Technologies focusing on human capturing, reconstruction, and digitization. Previously, she spent three years at USC-ICT as a postdoctoral researcher and research associate in the Vision and Graphics Lab lead by Dr.
 Hao Li. She obtained her Ph.D. from the University of Kentucky under the supervision of Dr. Ruigang Yang.<br/>
							  <b>Abstract: </b>High-fidelity avatar creation for films and games is tied with complex capture equipment, massive data, a long production cycle, and intensive manual labor by a production team. And it may still be in the notorious Uncanny Valley. In this talk, we will explore how to produce a lifelike avatar in hours using only an online video sequence. We will show how to leverage deep learning networks to accelerate and simplify the industrial avatar production procedure from data capturing to animation. And bring photorealism to the next level!<br/>
                </td>
              </tr>
            </table>
          </div>
        </div>
      </div>
			     
			     
			     
      <!-- Organizing Committee -->
      <div>

        <div>
          <h2 id="ORGANIZINGCOMMITTEES" class="elementor-heading-title">ORGANIZERS</h2>
        </div>

        <div class="elementor-widget-text-editor">
          <table class="tg">
            <tr>

              <td class="elementor-widget-image-box">
                <div class="elementor-image-box-wrapper">

                  <a href="https://www.yajie-zhao.com/" target="_blank">
                   <div class="elementor-image-box-img">
                      <img style="margin: auto; height: 150px; max-width: 150px;border-radius: 50%;"
                           src="img/yajie.jpg" alt=""> 
					
                 </div>

                    <div class="elementor-image-box-content">
                      <h3 class="elementor-image-box-title">Yajie Zhao</h3>
                      <p class="elementor-image-box-description">USC Institute for Creative Technologies,USA</p>
                    </div>
                  </a>
                </div>
              </td>
			  
			  <td class="elementor-widget-image-box">
                <div class="elementor-image-box-wrapper">

                  <a href="http://fdanieau.free.fr/" target="_blank">
                    <div class="elementor-image-box-img">
                      <img style="margin: auto; height: 150px;max-width: 150px;border-radius: 50%;"
                           src="img/fabien_danieau.jpg" alt="">
                    </div>

                    <div class="elementor-image-box-content">
                      <h3 class="elementor-image-box-title">Fabien Danieau</h3>
                      <p class="elementor-image-box-description">InterDigital, France</p>
                    </div>
                  </a>

                </div>
              </td>

              <td class="elementor-widget-image-box">
                <div class="elementor-image-box-wrapper">

                  <a href="https://stevetonneau.fr/" target="_blank">
                    <div class="elementor-image-box-img">
                      <img style="margin: auto; height: 150px;max-width: 150px;border-radius: 50%;"
                           src="img/stevetonneau.png" alt="">
                    </div>

                    <div class="elementor-image-box-content">
                      <h3 class="elementor-image-box-title">Steve Tonneau</h3>
                      <p class="elementor-image-box-description">University of Edinburgh, UK </p>
                    </div>
                  </a>

                </div>
              </td>

              

            </tr>
            <tr>

   
            </tr>
          </table>
        </div>
      </div>


      <!-- Contact Us -->
      <div>

        <div>
          <h2 id="CONTACT" class="elementor-heading-title">CONTACT US</h2>
        </div>
		 <div class="elementor-widget-text-editor">
           If you have any questions, please contact <b>Yajie Zhao</b> (zhao[at].ict.usc.edu), or <b>Fabien Danieau</b> (fabien.danieau[at]interdigital.com)
		   or <b>Steve Tonneau</b> (stonneau[at]ed.ac.uk).
		   <br><br>
       </div>
      
      </div>

    </div>

    <!-- footer -->
    <footer id="colophon" class="site-footer" role="contentinfo">
      <div class="wrap">

        <div class="widget-column footer-widget-1">
          <div class="textwidget"><p>CRDH Workshop - IEEE AIVR 2020</p>
          </div>
        </div>
    

      </div><!-- .wrap -->

    </footer><!-- #colophon -->

  </div><!-- .site-content-contain -->
</div><!-- #page -->

<script src="js/skip-link-focus-fix.js"></script>
<script src="js/navigation.js"></script>
<script src="js/global.js"></script>
<script src="js/jquery.scrollTo.js"></script>
<script src="js/wp-embed.min.js"></script>
<script src="js/wp-a11y.min.js"></script>
<script src="js/wp-custom-header.min.js"></script>
<script src="js/position.min.js"></script>
<script src="js/dialog.min.js"></script>
<script src="js/waypoints.min.js"></script>
<script src="js/swiper.jquery.min.js"></script>
<script src="js/frontend.min.js"></script>

</body>
</html>
